# MongoDB Query Translator with Ollama

A production-ready AI-powered system that converts natural language queries into optimized MongoDB queries using local Ollama models - no external API keys required!

## ğŸš€ Features

- **Natural Language Processing**: Convert English to MongoDB queries
- **Local AI**: Uses Ollama with gpt-oss:20b model (no API costs!)
- **Intelligent Validation**: Self-correcting query validation system
- **Production Ready**: Complete REST API with Docker support
- **Self-Contained**: No external dependencies or API keys
- **Privacy-First**: All data stays local

## ğŸ“ Project Structure

```
nl_to_mongo_new/
â”œâ”€â”€ api/                    # FastAPI backend server
â”œâ”€â”€ config/                 # Database and application configuration
â”œâ”€â”€ docs/                   # Documentation
â”œâ”€â”€ prompts/                # LLM prompt templates
â”œâ”€â”€ schema/                 # Schema discovery and analysis
â”œâ”€â”€ scripts/                # Utility scripts and deployment
â”œâ”€â”€ services/               # Core business logic
â”œâ”€â”€ tests/                  # Test suite
â”œâ”€â”€ utils/                  # Utility functions
â”œâ”€â”€ .env                    # Environment variables
â”œâ”€â”€ requirements.txt        # Python dependencies
â””â”€â”€ README.md              # This file
```

## ğŸ› ï¸ Quick Start

### Docker (Recommended)
```bash
# Clone and start
git clone <repository-url>
cd nl_to_mongo_new
cp env.example .env
# Edit .env with your MongoDB URI

# Deploy with Ollama
./scripts/deploy.sh
```

### Manual Setup
```bash
# Start Ollama
docker-compose up -d ollama

# Pull the model
docker exec -it ollama ollama pull gpt-oss:20b

# Start the application
docker-compose up -d mongodb-query-translator
```

### Local Development
```bash
# Install dependencies
pip install -r requirements.txt

# Start Ollama locally
ollama serve

# Pull the model
ollama pull gpt-oss:20b

# Start server
python scripts/start_server.py
```

## ğŸ¯ Usage

### API Endpoints

The server runs on `http://localhost:8000` with these endpoints:

- **GET** `/` - API information
- **GET** `/health` - System health check
- **POST** `/query/generate` - Generate MongoDB queries
- **POST** `/query/execute` - Execute queries
- **GET** `/database/info` - Database information

### Example Usage

```bash
# Health check
curl http://localhost:8000/health

# Generate a query
curl -X POST http://localhost:8000/query/generate \
  -H "Content-Type: application/json" \
  -d '{"query": "Find all active users", "execute_query": false}'
```

## ğŸ§ª Testing

Run the test suite:

```bash
# Run basic tests
python tests/test_basic.py

# Run all tests
python scripts/run_tests.py
```

## ğŸ“Š System Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Natural       â”‚    â”‚   FastAPI       â”‚    â”‚   MongoDB       â”‚
â”‚   Language      â”‚â”€â”€â”€â–¶â”‚   Backend       â”‚â”€â”€â”€â–¶â”‚   Database      â”‚
â”‚   Input         â”‚    â”‚   (Port 8000)   â”‚    â”‚                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                              â”‚
                              â–¼
                       â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                       â”‚   Ollama        â”‚
                       â”‚   gpt-oss:20b   â”‚
                       â”‚   (Port 11434)  â”‚
                       â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## ğŸ”§ Configuration

### Environment Variables

- `MONGODB_URI`: MongoDB Atlas connection string
- `OLLAMA_URL`: Ollama server URL (default: http://localhost:11434)
- `OLLAMA_MODEL`: Model name (default: gpt-oss:20b)
- `ATLAS_DATABASE_NAME`: Database name

### Dependencies

- **FastAPI**: Web framework
- **PyMongo**: MongoDB driver
- **Ollama**: Local LLM integration
- **Requests**: HTTP client

## ğŸ“ˆ Performance

- **Query Generation**: < 3 seconds (local inference)
- **Validation**: < 1 second
- **API Response**: < 4 seconds
- **No API Costs**: Completely free to run

## ğŸ† Benefits of Using Ollama

1. **No API Costs**: Completely free to run
2. **Privacy**: All data stays local
3. **Offline**: Works without internet
4. **Customizable**: Use any model you want
5. **Fast**: Local inference is very fast
6. **Self-contained**: No external dependencies

## ğŸ¤ Contributing

1. Fork the repository
2. Create a feature branch
3. Make your changes
4. Add tests
5. Submit a pull request

## ğŸ“„ License

This project is licensed under the MIT License.

## ğŸ†˜ Support

For support and questions:
- Check the documentation in `docs/`
- Run the test suite to verify functionality
- Review the API endpoints for usage examples

## ğŸ‰ Acknowledgments

Built with local AI using Ollama and gpt-oss:20b model for complete privacy and cost-effectiveness.
